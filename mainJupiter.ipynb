{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Uporabnik\\.cache\\kagglehub\\datasets\\karakaggle\\kaggle-cat-vs-dog-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.datasets.folder import IMG_EXTENSIONS\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Albumentations transforms\n",
    "basic_transform = A.Compose([\n",
    "    A.Resize(64, 64),\n",
    "    A.RandomCrop(64, 64),\n",
    "    ToTensorV2()\n",
    "])\n",
    "advanced_transform = A.Compose([\n",
    "    A.Resize(64, 64),\n",
    "    A.RandomCrop(64, 64),\n",
    "    A.HorizontalFlip(),\n",
    "    A.RandomBrightnessContrast(),\n",
    "    A.RandomRotate90(),\n",
    "    A.HueSaturationValue(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Albumentations wrapper\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = np.array(img)  # Convert from PIL image to NumPy array\n",
    "        augmented = self.transform(image=img)\n",
    "        return augmented[\"image\"]\n",
    "\n",
    "# Filtered ImageFolder to handle invalid files\n",
    "class FilteredImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        super().__init__(root, transform, target_transform)\n",
    "        valid_samples = []\n",
    "        for path, class_idx in self.samples:\n",
    "            try:\n",
    "                # Try opening the image to check for validity\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")  # Suppress warnings\n",
    "                    img = Image.open(path)\n",
    "                    img.verify()  # Verify image integrity\n",
    "                valid_samples.append((path, class_idx))\n",
    "            except (IOError, SyntaxError, ValueError) as e:\n",
    "                print(f\"Invalid image file {path}: {e}\")\n",
    "        self.samples = valid_samples\n",
    "        self.targets = [s[1] for s in self.samples]\n",
    "\n",
    "# Combined dataset for basic and advanced transforms\n",
    "class CombinedDataset(Dataset):\n",
    "    def __init__(self, dataset_path, basic_transform, advanced_transform):\n",
    "        self.image_folder = FilteredImageFolder(root=dataset_path)\n",
    "        self.basic_transform = AlbumentationsTransform(basic_transform)\n",
    "        self.advanced_transform = AlbumentationsTransform(advanced_transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 * len(self.image_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx % len(self.image_folder)\n",
    "        img, label = self.image_folder[original_idx]\n",
    "\n",
    "        if idx < len(self.image_folder):\n",
    "            img = self.basic_transform(img)\n",
    "        else:\n",
    "            img = self.advanced_transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Dataset path\n",
    "dataset_path = os.path.join(\n",
    "    'C:\\\\Users\\\\Uporabnik\\\\.cache\\\\kagglehub\\\\datasets\\\\karakaggle\\\\kaggle-cat-vs-dog-dataset\\\\versions\\\\1\\\\kagglecatsanddogs_3367a',\n",
    "    'PetImages'\n",
    ")\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "combined_dataset = CombinedDataset(dataset_path, basic_transform, advanced_transform)\n",
    "train_size = int(0.9 * len(combined_dataset))\n",
    "val_size = len(combined_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 64, 64]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# show img and imgs\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def show_image(image, label, classes):\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(classes[label])\n",
    "    plt.show()\n",
    "def show_images(dataloader, dataset):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    grid = make_grid(images, nrow=8)\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.title([dataset.image_folder.classes[i] for i in labels])\n",
    "    plt.show()\n",
    "\n",
    "# Fetch Batcha\n",
    "train_features_batch, train_labels_batch = next(iter(train_dataloader))\n",
    "print(train_features_batch.shape, train_labels_batch.shape)\n",
    "\n",
    "#show_image(train_features_batch[0], train_labels_batch[0], combined_dataset.image_folder.classes)\n",
    "#show_images(train_dataloader, combined_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_dataloader is 1404 with batches of size 32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of train_dataloader is {len(train_dataloader)} with batches of size {train_dataloader.batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "class DogOrCatModelV0(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_shape: int, \n",
    "                 hidden_units: int, \n",
    "                 output_shape: int):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                      out_channels=hidden_units, \n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            #nn.BatchNorm2d(hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(hidden_units), # samo normalizira, ne spreminja oblike. Ni nekaj pomembnega, lahko izpustimo samo I think da bi bli podatki slabši oz. tak je razloženo, nimam časa naštudirati njegove podrobnosti\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        second_hidden_units = hidden_units * 2\n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                      out_channels=second_hidden_units, \n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            #nn.BatchNorm2d(second_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=second_hidden_units, \n",
    "                      out_channels=second_hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(second_hidden_units), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        third_hidden_units = hidden_units * 4\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=second_hidden_units, \n",
    "                      out_channels=third_hidden_units, \n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            #nn.BatchNorm2d(third_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=third_hidden_units, \n",
    "                      out_channels=third_hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(third_hidden_units),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Fully connected layer; in_features is calculated dynamically\n",
    "            nn.Linear(in_features=hidden_units * 4 * 8 * 8,# na začetku dat na hidden_units*0, da najdemo napake kar se tiče dimenzij, potem pa spremenimo\n",
    "                      out_features=128), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, output_shape)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.block_1(x)\n",
    "        #print(f\"E: {x.shape}\")\n",
    "        x = self.block_2(x)\n",
    "        #print(f\"E: {x.shape}\")\n",
    "        x = self.block_3(x)\n",
    "        #print(f\"E: {x.shape}\")\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = DogOrCatModelV0(input_shape=3, hidden_units=16, output_shape=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to see if it works\n",
    "torch.manual_seed(42)\n",
    "\n",
    "images = torch.randn(size=(1, 3, 64, 64))\n",
    "# model(images) # zagon in izpis modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcija za izračun točnosti\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (torch.Tensor): Truth labels for predictions.\n",
    "        y_pred (torch.Tensor): Predictions to be compared to predictions.\n",
    "\n",
    "    Returns:\n",
    "        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n",
    "    \"\"\"\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss in optimizacija\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuda, če je na voljo\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#device # izpis naprave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# Set the log directory for TensorBoard\n",
    "log_dir = os.path.join(os.getcwd(), 'runs')  # Full path to the 'runs' folder\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test step\n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               accuracy_fn,\n",
    "               device: torch.device = device,\n",
    "               epoch: int = 0):\n",
    "    train_loss, train_acc = 0, 0\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(data_loader): # Iti skozi vse batche\n",
    "        X = X.float()\n",
    "        # Podatke na napravo\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        # Loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "         # Dodaj loss v train_loss\n",
    "        train_loss += loss.item()\n",
    "        # Kalkulacija točnosti\n",
    "        train_acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1))\n",
    "\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Loss avg za batch\n",
    "    train_loss /= len(val_dataloader)\n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    # Točnost avg za batch\n",
    "    train_acc /= len(val_dataloader)\n",
    "    \n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Training Acc: {train_acc:.2f}%\")\n",
    "\n",
    "\n",
    "def test_step(data_loader: torch.utils.data.DataLoader,\n",
    "              model: torch.nn.Module,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              accuracy_fn,\n",
    "              device: torch.device = device,\n",
    "              epoch: int = 0):\n",
    "    test_loss, test_acc = 0, 0\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            X = X.float()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            test_pred = model(X)\n",
    "\n",
    "            test_loss += loss_fn(test_pred, y).item()\n",
    "            test_acc += accuracy_fn(y_true=y, \n",
    "                                    y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        test_loss /= len(data_loader)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        test_acc /= len(data_loader)\n",
    "        print(f\"Test loss: {test_loss:.5f} | Test Acc: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "-------------------------------\n",
      "\n",
      "Train loss: 6.42745 | Training Acc: 488.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [07:08<00:00, 428.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.67016 | Test Acc: 58.57%\n",
      "=> Saving checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [07:08<00:00, 428.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 428.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Training setup\n",
    "train_time_start = timer()\n",
    "epochs = 1\n",
    "load_model = False\n",
    "\n",
    "if load_model:\n",
    "    try:\n",
    "        checkpoint = torch.load(\"checkpoint_9_1.pth.tar\")\n",
    "        load_checkpoint(checkpoint)\n",
    "    except FileNotFoundError:\n",
    "        print(\"Checkpoint file not found. Starting from scratch.\")\n",
    "else:\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "\n",
    "        # Perform training step and get the training loss\n",
    "        train_loss = train_step(\n",
    "            model=model, \n",
    "            data_loader=train_dataloader, \n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            accuracy_fn=accuracy_fn,\n",
    "            device=device,\n",
    "            epoch=epoch\n",
    "        )\n",
    "        \n",
    "        # Perform testing step and get the test loss\n",
    "        test_loss = test_step(\n",
    "            model=model,\n",
    "            data_loader=val_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            accuracy_fn=accuracy_fn,\n",
    "            device=device,\n",
    "            epoch=epoch\n",
    "        )\n",
    "        \n",
    "        checkpoint = {\n",
    "            'state_dict': model.state_dict(), \n",
    "            'optimizer': optimizer.state_dict()\n",
    "        }\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            save_checkpoint(checkpoint, filename=f\"checkpoint_{epoch+1}.pth.tar\")\n",
    "        \n",
    "        writer.flush()\n",
    "\n",
    "    # Calculate and print training time\n",
    "    train_time_end = timer()\n",
    "    print(f\"Training time: {train_time_end - train_time_start:.2f}s\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n",
    "        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n",
    "        loss_fn (torch.nn.Module): The loss function of model.\n",
    "        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n",
    "\n",
    "    Returns:\n",
    "        (dict): Results of model making predictions on data_loader.\n",
    "    \"\"\"\n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # Make predictions with the model\n",
    "            X = X.float()\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'DogOrCatModelV0',\n",
       " 'model_loss': 0.668354332447052,\n",
       " 'model_acc': 59.39503205128205}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model results on test dataset\n",
    "model_0_results = eval_model(model=model, data_loader=val_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0694, 0.4352]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "image_path = \"test_imgs/11.jpg\"\n",
    "image = torchvision.io.read_image(image_path).type(torch.float32) / 255.0\n",
    "\n",
    "min_dim = min(image.shape[1], image.shape[2])  # image.shape is (C, H, W)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(min_dim),  # Crop to the smallest dimension (1:1 ratio)\n",
    "    transforms.Resize((64, 64))      # Resize to 64x64\n",
    "])\n",
    "image = image_transform(image)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    custom_image_prediction = model(image.unsqueeze(0).to(device)) # Add batch dimension, make sure it's on the right device\n",
    "\n",
    "\"\"\"\n",
    "# show the image with the label and no grid\n",
    "def show_image(image, label, classes):\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    plt.title(classes[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Show the image with the prediction\n",
    "show_image(image, custom_image_prediction.argmax(dim=1).item(), combined_dataset.image_folder.classes)\n",
    "\"\"\"\n",
    "\n",
    "custom_image_prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
